{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-02-26T10:09:42.624224Z","iopub.status.busy":"2022-02-26T10:09:42.623122Z","iopub.status.idle":"2022-02-26T10:09:42.629725Z","shell.execute_reply":"2022-02-26T10:09:42.628397Z","shell.execute_reply.started":"2022-02-26T10:09:42.624166Z"}},"source":["# <center>Multiclass Vegetable Classification Using Transfer Learning</center>\n","<center><img src= \"https://p4.wallpaperbetter.com/wallpaper/667/254/333/vegetables-fruit-still-life-food-wallpaper-preview.jpg\" alt =\"Titanic\" style='width:500px;'></center><br>"]},{"cell_type":"markdown","metadata":{},"source":["<h1><center> Idea behind using Transfer learning </center></h1> \n","\n","- <h2> Instead of training a deep network from scratch, we can actually take a pre-trained network and use it for a different task as learning of a new task relies on the previously learned tasks. </h2><br>\n","- <h2> Assisting in image analysis and classification tasks including object detection with good accuracy.</h2><br>\n","- <h2> We will use InceptionV3 which is a network already trained on more than a million images from the ImageNet database </h2>"]},{"cell_type":"markdown","metadata":{},"source":["<https://colab.research.google.com/drive/1zLJ9X7iqv49Q3N6NtbEVcVvV6kKBdr5S#scrollTo=gNefaxekSaWR>"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T03:49:12.207482Z","iopub.status.busy":"2022-05-12T03:49:12.207137Z","iopub.status.idle":"2022-05-12T03:49:12.216489Z","shell.execute_reply":"2022-05-12T03:49:12.215516Z","shell.execute_reply.started":"2022-05-12T03:49:12.207438Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import seaborn as sns\n","from PIL import Image \n","from PIL import ImageEnhance\n","from skimage.io import imread\n","import matplotlib.pyplot as plt\n","\n","import os, random, pathlib, warnings, itertools, math\n","warnings.filterwarnings(\"ignore\")\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from sklearn.metrics import confusion_matrix\n","\n","from tensorflow.keras import models\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.inception_v3 import InceptionV3,preprocess_input\n","from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dense, Dropout\n","\n","K.clear_session()"]},{"cell_type":"markdown","metadata":{},"source":["# <span style=\"color:blue\"><b>1. Loading the dataset</b></span>\n","<hr>\n","<h2> Let's define the path to base directories used in the project </h2>"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T03:49:18.127518Z","iopub.status.busy":"2022-05-12T03:49:18.127062Z","iopub.status.idle":"2022-05-12T03:49:18.132529Z","shell.execute_reply":"2022-05-12T03:49:18.131420Z","shell.execute_reply.started":"2022-05-12T03:49:18.127477Z"},"trusted":true},"outputs":[],"source":["dataset='../input/vegetable-image-dataset/Vegetable Images'\n","\n","train_folder = os.path.join(dataset,\"train\")\n","test_folder = os.path.join(dataset,\"validation\")\n","validation_folder = os.path.join(dataset,\"test\")"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Counting number of images in a folder. (test set in our case)</h2>"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T03:49:21.764292Z","iopub.status.busy":"2022-05-12T03:49:21.763510Z","iopub.status.idle":"2022-05-12T03:49:22.947077Z","shell.execute_reply":"2022-05-12T03:49:22.946403Z","shell.execute_reply.started":"2022-05-12T03:49:21.764251Z"},"trusted":true},"outputs":[],"source":["def count_files(rootdir):\n","    '''counts the number of files in each subfolder in a directory'''\n","    for path in pathlib.Path(rootdir).iterdir():\n","        if path.is_dir():\n","            print(\"There are \" + str(len([name for name in os.listdir(path) \\\n","            if os.path.isfile(os.path.join(path, name))])) + \" files in \" + \\\n","            str(path.name))\n"," \n","count_files(os.path.join(test_folder))"]},{"cell_type":"markdown","metadata":{},"source":["<h2> As evident, Dataset is well balanced with each class containing : </h2>\n","    \n","- <h3> 1000 images for training set. </h3>\n","    \n","- <h3> 200 images for test set. </h3>"]},{"cell_type":"markdown","metadata":{},"source":["# <span style=\"color:blue\"><b>2. Image Processing</b></span>\n","<hr>\n","<h2> The goal of image processing is improvement of pictorial information for human interpretation. Basic manipulation and filtering can lead to increased understanding for feature extraction as well. </h2><br>\n","<h3><b><span style=\"color:green\">Here we can select any vegetable of our choice and a random image from the class is displayed with comparison to a processed image.</span></b></h3>\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T03:49:26.370143Z","iopub.status.busy":"2022-05-12T03:49:26.369846Z","iopub.status.idle":"2022-05-12T03:49:27.554767Z","shell.execute_reply":"2022-05-12T03:49:27.554146Z","shell.execute_reply.started":"2022-05-12T03:49:26.370110Z"},"trusted":true},"outputs":[],"source":["image_folder=\"Cucumber\"  # The vegetable you want to display\n","number_of_images=2       # Number of images to display\n","\n","def Preprocess():\n","    j=1\n","    for i in range(number_of_images):\n","    \n","        folder = os.path.join(test_folder,image_folder)\n","        a=random.choice(os.listdir(folder))\n","\n","        image=Image.open(os.path.join(folder,a))\n","        image_duplicate=image.copy()\n","        plt.figure(figsize=(10,10))\n","\n","        plt.subplot(number_of_images,2,j)\n","        plt.title(label='Orignal', size=17, pad='7.0', loc=\"center\", fontstyle='italic')\n","        plt.imshow(image)\n","        j+=1\n","\n","        image1=ImageEnhance.Color(image_duplicate).enhance(1.35)\n","        image1=ImageEnhance.Contrast(image1).enhance(1.45)\n","        image1=ImageEnhance.Sharpness(image1).enhance(2.5)\n","        \n","        plt.subplot(number_of_images,2,j)\n","        plt.title(label='Processed', size=17, pad='7.0', loc=\"center\", fontstyle='italic')\n","        plt.imshow(image1)\n","        j+=1\n","        \n","Preprocess()"]},{"cell_type":"markdown","metadata":{},"source":["<h3> We increased the color saturation, contrast and finally sharpened the image for drawing texture and viewer focus. The image after processing looks appealing and brighter</h3>"]},{"cell_type":"markdown","metadata":{},"source":["# <span style=\"color:blue\"><b> 3. Data Visualization (EDA)</b></span>\n","<hr>\n","<h2> We can start exploring the dataset and visualize any class label <i>(for instance, Capsicum)</i>. You can choose any vegetable to visualize the images of that class. Changing rows and columns variable also results in different format positioning of matplotlib. </h2>"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T03:49:32.001396Z","iopub.status.busy":"2022-05-12T03:49:32.001092Z","iopub.status.idle":"2022-05-12T03:49:32.861763Z","shell.execute_reply":"2022-05-12T03:49:32.860914Z","shell.execute_reply.started":"2022-05-12T03:49:32.001363Z"},"trusted":true},"outputs":[],"source":["select_vegetable=\"Capsicum\"\n","rows,columns = 1,5\n","\n","display_folder=os.path.join(train_folder,select_vegetable)\n","total_images=rows*columns\n","fig=plt.figure(1, figsize=(20, 10))\n","\n","for i,j in enumerate(os.listdir(display_folder)):      \n","    \n","    img = plt.imread(os.path.join(train_folder,select_vegetable,j))\n","    fig=plt.subplot(rows, columns, i+1)\n","    fig.set_title(select_vegetable, pad = 11, size=20)\n","    plt.imshow(img)\n","    \n","    if i==total_images-1:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["<h2> Now let's visualize the whole dataset by picking a random image from each class inside training dataset. </h2>"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T03:49:37.296882Z","iopub.status.busy":"2022-05-12T03:49:37.296576Z","iopub.status.idle":"2022-05-12T03:49:39.797143Z","shell.execute_reply":"2022-05-12T03:49:39.795862Z","shell.execute_reply.started":"2022-05-12T03:49:37.296852Z"},"trusted":true},"outputs":[],"source":["images = []\n","\n","for food_folder in sorted(os.listdir(train_folder)):\n","    food_items = os.listdir(train_folder + '/' + food_folder)\n","    food_selected = np.random.choice(food_items)\n","    images.append(os.path.join(train_folder,food_folder,food_selected))\n","                                     \n","fig=plt.figure(1, figsize=(15, 10))\n","\n","for subplot,image_ in enumerate(images):\n","    category=image_.split('/')[-2]\n","    imgs = plt.imread(image_)\n","    a,b,c=imgs.shape\n","    fig=plt.subplot(3, 5, subplot+1)\n","    fig.set_title(category, pad = 10,size=18)\n","    plt.imshow(imgs)\n","    \n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"font-size:18px;color:green\"><b>There are 15 vegetables (output classes) and one random image from each class helps in determining basic outlook of dataset and what picture quality along with different metric are visible. So far, So Good! </b></span>"]},{"cell_type":"markdown","metadata":{},"source":["# <span style=\"color:blue\"><b>4. Model Building</b></span>\n","<hr>\n","<h2> Let's start building the transfer learning network to train our model using <u>InceptionV3</u>. </h2>"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T05:19:50.824113Z","iopub.status.busy":"2022-05-12T05:19:50.823813Z","iopub.status.idle":"2022-05-12T05:19:52.611244Z","shell.execute_reply":"2022-05-12T05:19:52.610618Z","shell.execute_reply.started":"2022-05-12T05:19:50.824083Z"},"trusted":true},"outputs":[],"source":["# print(images)\n","IMAGE_SIZE = [224, 224]\n","\n","inception = InceptionV3(input_shape=(250,250,3), weights=None, include_top=False)\n","\n","for layer in inception.layers:\n","    layer.trainable = False\n","\n","x = inception.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128,activation='relu')(x)\n","x = Dropout(0.2)(x)\n","\n","prediction = Dense(15, activation='softmax')(x)\n","\n","model = Model(inputs=inception.input, outputs=prediction)\n","\n","model.compile(\n","  loss='categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-02-26T11:20:33.110948Z","iopub.status.busy":"2022-02-26T11:20:33.109805Z","iopub.status.idle":"2022-02-26T11:20:33.120429Z","shell.execute_reply":"2022-02-26T11:20:33.119254Z","shell.execute_reply.started":"2022-02-26T11:20:33.110892Z"}},"source":["# <span style=\"color:blue\"><b>5. Model Training</b></span>\n","<hr>"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:12:00.306268Z","iopub.status.busy":"2022-05-12T07:12:00.305847Z","iopub.status.idle":"2022-05-12T07:12:01.255523Z","shell.execute_reply":"2022-05-12T07:12:01.254194Z","shell.execute_reply.started":"2022-05-12T07:12:00.306237Z"},"trusted":true},"outputs":[],"source":["train_datagen = image.ImageDataGenerator(rescale = 1./255,\n","                                         shear_range = 0.2,\n","                                         zoom_range = 0.2,\n","                                         horizontal_flip = True)\n","\n","test_datagen = image.ImageDataGenerator(rescale = 1./255)\n","\n","training_set = train_datagen.flow_from_directory(\n","    train_folder,\n","    target_size = (250, 250),\n","    batch_size = 64,\n","    class_mode = 'categorical')\n","\n","test_set = test_datagen.flow_from_directory(\n","    test_folder, \n","    target_size = (250,250),\n","    batch_size = 64, \n","    class_mode = 'categorical')"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:12:12.470357Z","iopub.status.busy":"2022-05-12T07:12:12.469542Z","iopub.status.idle":"2022-05-12T07:12:12.477031Z","shell.execute_reply":"2022-05-12T07:12:12.476400Z","shell.execute_reply.started":"2022-05-12T07:12:12.470287Z"},"trusted":true},"outputs":[],"source":["class_map = training_set.class_indices\n","class_map"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T05:20:02.199421Z","iopub.status.busy":"2022-05-12T05:20:02.199004Z","iopub.status.idle":"2022-05-12T06:31:03.968487Z","shell.execute_reply":"2022-05-12T06:31:03.967179Z","shell.execute_reply.started":"2022-05-12T05:20:02.199383Z"},"trusted":true},"outputs":[],"source":["r = model.fit_generator(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=5,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the Model"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:06:13.555553Z","iopub.status.busy":"2022-05-12T07:06:13.555267Z","iopub.status.idle":"2022-05-12T07:06:14.193187Z","shell.execute_reply":"2022-05-12T07:06:14.192142Z","shell.execute_reply.started":"2022-05-12T07:06:13.555523Z"},"trusted":true},"outputs":[],"source":["model.save('model_inceptionV3_epoch5.h5')"]},{"cell_type":"markdown","metadata":{},"source":["## Accuracy and Loss Curves"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:12:24.142569Z","iopub.status.busy":"2022-05-12T07:12:24.142279Z","iopub.status.idle":"2022-05-12T07:12:24.694366Z","shell.execute_reply":"2022-05-12T07:12:24.693591Z","shell.execute_reply.started":"2022-05-12T07:12:24.142540Z"},"trusted":true},"outputs":[],"source":["def plot_accuracy(history):\n","    \n","    plt.plot(history.history['accuracy'],label='train accuracy')\n","    plt.plot(history.history['val_accuracy'],label='validation accuracy')\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='best')\n","    plt.savefig('Accuracy_v1_InceptionV3')\n","    plt.show()\n","    \n","def plot_loss(history):\n","    \n","    plt.plot(history.history['loss'],label=\"train loss\")\n","    plt.plot(history.history['val_loss'],label=\"validation loss\")\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='best')\n","    plt.savefig('Loss_v1_InceptionV3')\n","    plt.show()\n","    \n","plot_accuracy(r)\n","plot_loss(r)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Layers"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:12:30.567960Z","iopub.status.busy":"2022-05-12T07:12:30.567378Z","iopub.status.idle":"2022-05-12T07:12:30.579579Z","shell.execute_reply":"2022-05-12T07:12:30.578670Z","shell.execute_reply.started":"2022-05-12T07:12:30.567922Z"},"trusted":true},"outputs":[],"source":["print(\"Total layers in the model : \",len(model.layers),\"\\n\")\n","\n","layers = [layer.output for layer in model.layers[0:]]\n","layer_names = []\n","for layer in model.layers[0:]: \n","    layer_names.append(layer.name)\n","    \n","print(\"First layer : \", layer_names[0])\n","print(\"InceptionV3 layers : Layer 2 to Layer 311\")\n","print(\"Our fine tuned layers : \", layer_names[311:314])\n","print(\"Final Layer : \", layer_names[314])\n"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-02-26T11:21:08.736589Z","iopub.status.busy":"2022-02-26T11:21:08.735808Z","iopub.status.idle":"2022-02-26T11:21:08.742209Z","shell.execute_reply":"2022-02-26T11:21:08.741201Z","shell.execute_reply.started":"2022-02-26T11:21:08.736554Z"}},"source":["# <b><span style=\"color:blue\">6. Predictions</span></b>\n","<hr>\n","<h2> Loading the model </h2>"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:12:33.928810Z","iopub.status.busy":"2022-05-12T07:12:33.928188Z","iopub.status.idle":"2022-05-12T07:12:36.746457Z","shell.execute_reply":"2022-05-12T07:12:36.745544Z","shell.execute_reply.started":"2022-05-12T07:12:33.928771Z"},"trusted":true},"outputs":[],"source":["K.clear_session()\n","path_to_model='./model_inceptionV3_epoch5.h5'\n","print(\"Loading the model..\")\n","model = load_model(path_to_model)\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the model on validation set"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-05-12T07:12:52.163419Z","iopub.status.busy":"2022-05-12T07:12:52.162991Z","iopub.status.idle":"2022-05-12T07:13:50.498199Z","shell.execute_reply":"2022-05-12T07:13:50.496767Z","shell.execute_reply.started":"2022-05-12T07:12:52.163386Z"},"trusted":true},"outputs":[],"source":["validation_data_dir = '../input/vegetable-image-dataset/Vegetable Images/test'\n","\n","validation_datagen = image.ImageDataGenerator(rescale=1. / 255)\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(250,250),\n","    batch_size=64,\n","    class_mode='categorical')\n","\n","scores = model.evaluate_generator(validation_generator)\n","print(\"Test Accuracy: {:.3f}\".format(scores[1]))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-02-28T12:44:40.456852Z","iopub.status.busy":"2022-02-28T12:44:40.456506Z","iopub.status.idle":"2022-02-28T12:44:40.476641Z","shell.execute_reply":"2022-02-28T12:44:40.475887Z","shell.execute_reply.started":"2022-02-28T12:44:40.456764Z"}},"source":["<h2> Functions to predict the output of our model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T07:09:17.524297Z","iopub.status.idle":"2022-05-12T07:09:17.524938Z","shell.execute_reply":"2022-05-12T07:09:17.524710Z","shell.execute_reply.started":"2022-05-12T07:09:17.524683Z"},"trusted":true},"outputs":[],"source":["category={\n","    0: 'Bean', 1: 'Bitter_Gourd', 2: 'Bottle_Gourd', 3 : 'Brinjal', 4: \"Broccoli\", 5: 'Cabbage', 6: 'Capsicum', 7: 'Carrot', 8: 'Cauliflower',\n","    9: 'Cucumber', 10: 'Papaya', 11: 'Potato', 12: 'Pumpkin', 13 : \"Radish\", 14: \"Tomato\"\n","}\n","\n","def predict_image(filename,model):\n","    img_ = image.load_img(filename, target_size=(224, 224))\n","    img_array = image.img_to_array(img_)\n","    img_processed = np.expand_dims(img_array, axis=0) \n","    img_processed /= 255.   \n","    \n","    prediction = model.predict(img_processed)\n","    index = np.argmax(prediction)\n","    \n","    plt.title(\"Prediction - {}\".format(category[index]))\n","    plt.imshow(img_array)\n","    \n","def predict_dir(filedir,model):\n","    cols=3\n","    pos=0\n","    images=[]\n","    total_images=len(os.listdir(filedir))\n","    rows=total_images//cols + 1\n","    \n","    true=filedir.split('/')[-1]\n","    \n","    for i in sorted(os.listdir(filedir)):\n","        images.append(os.path.join(filedir,i))\n","        \n","    for subplot, imggg in enumerate(images):\n","        img_ = image.load_img(imggg, target_size=(224, 224))\n","        img_array = image.img_to_array(img_)\n","        img_processed = np.expand_dims(img_array, axis=0) \n","        img_processed /= 255.\n","        prediction = model.predict(img_processed)\n","        index = np.argmax(prediction)\n","        \n","        pred=category.get(index)\n","        if pred==true:\n","            pos+=1\n","\n","    acc=pos/total_images\n","    print(\"Accuracy for {orignal}: {:.2f} ({pos}/{total})\".format(acc,pos=pos,total=total_images,orignal=true))"]},{"cell_type":"markdown","metadata":{},"source":["- <h3> Single image prediction </h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T03:19:18.024768Z","iopub.status.idle":"2022-05-12T03:19:18.025042Z","shell.execute_reply":"2022-05-12T03:19:18.024914Z","shell.execute_reply.started":"2022-05-12T03:19:18.024898Z"},"trusted":true},"outputs":[],"source":["predict_image(os.path.join(validation_folder,'Cauliflower/1064.jpg'),model)"]},{"cell_type":"markdown","metadata":{},"source":["- <h3> Validation directory accuracy prediction </h3>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T03:19:18.025972Z","iopub.status.idle":"2022-05-12T03:19:18.026279Z","shell.execute_reply":"2022-05-12T03:19:18.026131Z","shell.execute_reply.started":"2022-05-12T03:19:18.026117Z"},"trusted":true},"outputs":[],"source":["for i in os.listdir(validation_folder):\n","    predict_dir(os.path.join(validation_folder,i),model)"]},{"cell_type":"markdown","metadata":{},"source":["# <b><span style=\"color:blue\">7. Evaluation</span></b>\n","<hr> \n","<h2> Loading the model </h2>"]},{"cell_type":"markdown","metadata":{},"source":["## Confusion Matrix for evaluating the performance of our classification model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T03:19:18.027176Z","iopub.status.idle":"2022-05-12T03:19:18.027496Z","shell.execute_reply":"2022-05-12T03:19:18.027328Z","shell.execute_reply.started":"2022-05-12T03:19:18.027313Z"},"trusted":true},"outputs":[],"source":["def labels_confusion_matrix(validation_folder):\n","    \n","    folder_path=validation_folder\n","    \n","    mapping={}\n","    for i,j in enumerate(sorted(os.listdir(folder_path))):\n","        mapping[j]=i\n","    \n","    files=[]\n","    real=[]\n","    predicted=[]\n","\n","    for i in os.listdir(folder_path):\n","        \n","        true=os.path.join(folder_path,i)\n","        true=true.split('/')[-1]\n","        true=mapping[true]\n","        \n","        for j in os.listdir(os.path.join(folder_path,i)):\n","\n","            img_ = image.load_img(os.path.join(folder_path,i,j), target_size=(224,224))\n","            img_array = image.img_to_array(img_)\n","            img_processed = np.expand_dims(img_array, axis=0) \n","            img_processed /= 255.\n","            prediction = model.predict(img_processed)\n","            index = np.argmax(prediction)\n","\n","            predicted.append(index)\n","            real.append(true)\n","            \n","    return (real,predicted)\n","\n","def print_confusion_matrix(real,predicted):\n","    total_output_labels = 15\n","    cmap=\"turbo\"\n","    cm_plot_labels = [i for i in range(15)]\n","    \n","    cm = confusion_matrix(y_true=real, y_pred=predicted)\n","    df_cm = pd.DataFrame(cm,cm_plot_labels,cm_plot_labels)\n","    sns.set(font_scale=1.2) # for label size\n","    plt.figure(figsize = (15,10))\n","    s=sns.heatmap(df_cm,fmt=\"d\", annot=True,cmap=cmap) # font size\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.savefig('confusion_matrix.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T03:19:18.02887Z","iopub.status.idle":"2022-05-12T03:19:18.029151Z","shell.execute_reply":"2022-05-12T03:19:18.029017Z","shell.execute_reply.started":"2022-05-12T03:19:18.029003Z"},"trusted":true},"outputs":[],"source":["y_true,y_pred=labels_confusion_matrix(validation_folder)\n","print_confusion_matrix(y_true,y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing all incorrect images predicted for a particular vegetable category by our classifier. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T03:19:18.030068Z","iopub.status.idle":"2022-05-12T03:19:18.030373Z","shell.execute_reply":"2022-05-12T03:19:18.030219Z","shell.execute_reply.started":"2022-05-12T03:19:18.030204Z"},"trusted":true},"outputs":[],"source":["def wrong_input_capture(test_category):\n","    \n","    a=os.path.basename(test_category)\n","    wrong_array=[]\n","    \n","    for i in os.listdir(test_category):\n","        \n","        imggg=os.path.join(test_category,i)\n","        \n","        img_ = image.load_img(imggg, target_size=(224, 224))\n","        img_array = image.img_to_array(img_)\n","        img_processed = np.expand_dims(img_array, axis=0) \n","        img_processed /= 255.\n","        prediction = model.predict(img_processed)\n","        \n","        index = np.argmax(prediction)\n","        pred=category.get(index)\n","        \n","        if not pred==a:\n","            wrong_array.append((imggg,pred))\n","            \n","    return wrong_array\n","    \n","def visualize_wrong_input(images):\n","    \n","    fig=plt.figure(1, figsize=(20, 25))\n","    total_images=len(images)\n","    rows=math.ceil(float(total_images/3))\n","    for subplot,(image_path,predicted) in enumerate(images):\n","        img = plt.imread(image_path)\n","        fig=plt.subplot(rows, 3, subplot+1)\n","        fig.set_title(\"Predicted - {}\".format(predicted), pad = 10,size=18)\n","        plt.imshow(img)\n","        \n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-12T03:19:18.031578Z","iopub.status.idle":"2022-05-12T03:19:18.031885Z","shell.execute_reply":"2022-05-12T03:19:18.031749Z","shell.execute_reply.started":"2022-05-12T03:19:18.031732Z"},"trusted":true},"outputs":[],"source":["vegetable=\"Broccoli\"\n","path=os.path.join(validation_folder,vegetable)\n","images= wrong_input_capture(path)\n","visualize_wrong_input(images)"]},{"cell_type":"markdown","metadata":{},"source":["<center><span style=\"font-size:21px;color:blue\"><b>As you can see, the false positives are really low as we have used transfer learning which has given us good accuracy!</b></span></center>\n","<hr>\n","<h2><center>If you liked this kernel, please consider upvoting.<br>Happy Kaggling.</center></h2>\n","\n","<hr>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
